{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is to experiment on Lexical features and Tf-Idf vvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surajr/anaconda2/lib/python2.7/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.spatial.distance as ds_metric\n",
    "from textblob import TextBlob, Word\n",
    "import gensim \n",
    "import io\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with io.open('sarcasm_dataset.txt','r') as fname:\n",
    "    file_content = fname.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to extract the below features.\n",
    "1. Presence of emoji's\n",
    "2. Count of number of Question marks\n",
    "3. Count of number of Exclamation marks\n",
    "4. presence of hashtags other than #sarcasm\n",
    "5. presence of any @user tweets\n",
    "\n",
    "\"\"\"\n",
    "def lexicalFeatures(tweet_content):\n",
    "    emoji = {\n",
    "    \"&lt;3\" : \"positive\",\":D\" : \"positive\",\t\":d\" : \"positive\", \":dd\" : \"positive\", \":P\" : \"positive\", \":p\" : \"positive\",\"8)\" : \"positive\",\n",
    "    \"8-)\" : \"positive\",  \":-)\" : \"positive\",    \":)\" : \"positive\",    \";)\" : \"positive\",    \"(-:\" : \"positive\",    \"(:\" : \"positive\",\n",
    "    \":')\" : \"positive\",    \"xD\" : \"positive\",    \"XD\" : \"positive\",  \"yay!\" : \"positive\",  \"yay\" : \"positive\",  \"yaay\" : \"positive\",\n",
    "    \"yaaay\" : \"positive\",  \"yaaaay\" : \"positive\", \"yaaaaay\" : \"positive\", \"Yay!\" : \"positive\", \"Yay\" : \"positive\", \"Yaay\" : \"positive\",\n",
    "    \"Yaaay\" : \"positive\", \"Yaaaay\" : \"positive\", \"Yaaaaay\" : \"positive\",  \":/\" : \"negative\", \"&gt;\" : \"negative\", \":'(\" : \"negative\",\n",
    "    \":-(\" : \"negative\", \":(\" : \"negative\", \":s\" : \"negative\",\":-s\" : \"negative\",\"-_-\" : \"negative\", \"-.-\" : \"negative\" }\n",
    "    lex_feature, lex_label, emoji_count = [], [],defaultdict(int)\n",
    "    for tweet_content in file_content:\n",
    "        isPresent, temp = False, []\n",
    "        tweet, label = tweet_content[:-2].split(), tweet_content[-2]        \n",
    "        \n",
    "        for word in tweet:\n",
    "            if word in emoji:            \n",
    "                isPresent = True   \n",
    "\n",
    "        temp.append(isPresent)\n",
    "        temp.append(int(tweet.count(\"?\")))\n",
    "        temp.append(int(tweet.count(\"!\")))\n",
    "        \n",
    "        other_hashtags=[j[1:] for j in tweet if j.startswith(\"#\")]\n",
    "        temp.append(1.0 if len(other_hashtags)!=0 else 0.0)\n",
    "        \n",
    "        lex_label.append(label)\n",
    "        lex_feature.append(temp)\n",
    "    return lex_feature, lex_label \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat, lex_label = lexicalFeatures(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat = pd.DataFrame(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoding=\"utf-8\"\n",
    "sentences = []\n",
    "for tweet_content in file_content:\n",
    "    tweet, label = tweet_content[:-2].split(), tweet_content[-2]\n",
    "    sentences.append(tweet)\n",
    "# train word2vec on the tweets\n",
    "model = gensim.models.Word2Vec(sentences, min_count=3)\n",
    "model.save('gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets add the features to TF-IDF and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.linear_model as lm\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import pipeline,metrics, grid_search \n",
    "from numpy import genfromtxt\n",
    "from sklearn import cross_validation, tree, linear_model\n",
    "import sklearn.ensemble as ek\n",
    "from sklearn import cross_validation, tree, linear_model\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "import sklearn.linear_model as lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>Blob Polarity</th>\n",
       "      <th>Blob Subjectivity</th>\n",
       "      <th>Capitalization</th>\n",
       "      <th>Negative Sentiment</th>\n",
       "      <th>POS_1</th>\n",
       "      <th>POS_2</th>\n",
       "      <th>POS_3</th>\n",
       "      <th>POS_4</th>\n",
       "      <th>...</th>\n",
       "      <th>first half Blob Subjectivity</th>\n",
       "      <th>first half sentiment</th>\n",
       "      <th>negative Sentiment first half</th>\n",
       "      <th>negative Sentiment second half</th>\n",
       "      <th>positive Sentiment first half</th>\n",
       "      <th>positive Sentiment second half</th>\n",
       "      <th>second half Blob Polarity</th>\n",
       "      <th>second half Blob Subjectivity</th>\n",
       "      <th>second half sentiment</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.80000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-0.638889</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.180556</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-1.138889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.50000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.80000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.778409</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-0.541667</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.153409</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>-0.632576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.601562</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.601562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  label  Blob Polarity  Blob Subjectivity  Capitalization  \\\n",
       "0           0      1        0.50000                0.6               0   \n",
       "1           1      1       -0.80000                0.9               0   \n",
       "2           2      0       -0.50000                1.0               0   \n",
       "3           3      0       -0.80000                0.9               1   \n",
       "4           4      1        0.78125                0.6               1   \n",
       "\n",
       "   Negative Sentiment  POS_1  POS_2  POS_3  POS_4    ...      \\\n",
       "0            0.031250    2.0    0.0    2.0    0.0    ...       \n",
       "1            1.444444    3.0    0.0    3.0    1.0    ...       \n",
       "2            0.725000    3.0    1.0    2.0    2.0    ...       \n",
       "3            0.778409    1.0    0.0    7.0    3.0    ...       \n",
       "4            0.039062    4.0    0.0    2.0    1.0    ...       \n",
       "\n",
       "   first half Blob Subjectivity  first half sentiment  \\\n",
       "0                           0.6              0.593750   \n",
       "1                           0.9             -0.638889   \n",
       "2                           1.0             -0.600000   \n",
       "3                           0.9             -0.541667   \n",
       "4                           0.6              0.601562   \n",
       "\n",
       "   negative Sentiment first half  negative Sentiment second half  \\\n",
       "0                       0.031250                        0.000000   \n",
       "1                       0.819444                        0.625000   \n",
       "2                       0.725000                        0.000000   \n",
       "3                       0.625000                        0.153409   \n",
       "4                       0.039062                        0.000000   \n",
       "\n",
       "   positive Sentiment first half  positive Sentiment second half  \\\n",
       "0                       0.625000                          0.0000   \n",
       "1                       0.180556                          0.1250   \n",
       "2                       0.125000                          0.0000   \n",
       "3                       0.083333                          0.0625   \n",
       "4                       0.640625                          0.0000   \n",
       "\n",
       "   second half Blob Polarity  second half Blob Subjectivity  \\\n",
       "0                        0.0                            0.0   \n",
       "1                        0.0                            0.0   \n",
       "2                        0.0                            0.0   \n",
       "3                        0.0                            0.0   \n",
       "4                        0.0                            0.0   \n",
       "\n",
       "   second half sentiment  sentiment  \n",
       "0               0.000000   0.593750  \n",
       "1              -0.500000  -1.138889  \n",
       "2               0.000000  -0.600000  \n",
       "3              -0.090909  -0.632576  \n",
       "4               0.000000   0.601562  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pandas.DataFrame()\n",
    "df = pandas.read_csv(\"feature_dataset.csv\", header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df.drop(['label'],axis=1).values\n",
    "Y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, Y ,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = pandas.DataFrame(X_train)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "\n",
    "X_test = pandas.DataFrame(X_test)\n",
    "X_test = X_test.fillna(X_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vec=TfidfVectorizer(analyzer=\"word\",max_features=None,strip_accents='unicode',token_pattern=r'\\w{1,}',lowercase=True,ngram_range=(1,3),min_df=2,use_idf=True,smooth_idf=True,norm=\"l2\",sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = { \"DecisionTree\":tree.DecisionTreeClassifier(max_depth=10),\n",
    "         \"RandomForest\":ek.RandomForestClassifier(n_estimators=50),\n",
    "         \"Adaboost\":ek.AdaBoostClassifier(n_estimators=50),\n",
    "         \"GradientBoosting\":ek.GradientBoostingClassifier(n_estimators=50),\n",
    "         \"GNB\":GaussianNB(),\n",
    "         \"LogisticRegression\":lm.LogisticRegression(C=0.30,penalty=\"l1\")   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression : 0.771929824561 \n",
      "RandomForest : 0.744360902256 \n",
      "GradientBoosting : 0.749373433584 \n",
      "GNB : 0.706766917293 \n",
      "DecisionTree : 0.676691729323 \n",
      "Adaboost : 0.729323308271 \n"
     ]
    }
   ],
   "source": [
    "results_algo = {}\n",
    "for algo in model:\n",
    "    clf = model[algo]\n",
    "    clf.fit(X_train,y_train.astype(int))\n",
    "    score = clf.score(X_test,y_test.astype(int))\n",
    "    print (\"%s : %s \" %(algo, score))\n",
    "    results_algo[algo] = score"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
